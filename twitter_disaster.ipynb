{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad88bb58",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "id": "1e1da99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from xgboost import XGBClassifier\n",
    "from collections import defaultdict\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "id": "de48220e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "0     Our Deeds are the Reason of this #earthquake M...       1  \n",
       "1                Forest fire near La Ronge Sask. Canada       1  \n",
       "2     All residents asked to 'shelter in place' are ...       1  \n",
       "3     13,000 people receive #wildfires evacuation or...       1  \n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1  \n",
       "...                                                 ...     ...  \n",
       "7608  Two giant cranes holding a bridge collapse int...       1  \n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1  \n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1  \n",
       "7611  Police investigating after an e-bike collided ...       1  \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1  \n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 601,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read train.csv\n",
    "df = pd.read_csv('./twitter/train.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49e1537",
   "metadata": {},
   "source": [
    "## Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "id": "60efd005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        7613 non-null   int64 \n",
      " 1   keyword   7552 non-null   object\n",
      " 2   location  5080 non-null   object\n",
      " 3   text      7613 non-null   object\n",
      " 4   target    7613 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 297.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "id": "244e3bc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7613.000000</td>\n",
       "      <td>7613.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5441.934848</td>\n",
       "      <td>0.42966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3137.116090</td>\n",
       "      <td>0.49506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2734.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5408.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8146.000000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10873.000000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id      target\n",
       "count   7613.000000  7613.00000\n",
       "mean    5441.934848     0.42966\n",
       "std     3137.116090     0.49506\n",
       "min        1.000000     0.00000\n",
       "25%     2734.000000     0.00000\n",
       "50%     5408.000000     0.00000\n",
       "75%     8146.000000     1.00000\n",
       "max    10873.000000     1.00000"
      ]
     },
     "execution_count": 603,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "id": "a0dfa800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id           int64\n",
       "keyword     object\n",
       "location    object\n",
       "text        object\n",
       "target       int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 604,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "id": "05f483cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id           0.0%\n",
       "keyword      0.8%\n",
       "location    33.3%\n",
       "text         0.0%\n",
       "target       0.0%\n",
       "dtype: object"
      ]
     },
     "execution_count": 605,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for missing values\n",
    "missing_per = 100*df.isna().sum()/len(df)\n",
    "missing_per = round(missing_per,1).astype(str) + '%'\n",
    "missing_per"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0382e63",
   "metadata": {},
   "source": [
    "## Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "id": "e3123a25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    4342\n",
       "1    3271\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 606,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# distribution of target variable\n",
    "df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca32cb6b",
   "metadata": {},
   "source": [
    "## Targets and Independent Variables(Locations and Keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "id": "d03030b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "location\n",
      "USA                104\n",
      "New York            71\n",
      "United States       50\n",
      "London              45\n",
      "Canada              29\n",
      "Nigeria             28\n",
      "UK                  27\n",
      "Los Angeles, CA     26\n",
      "India               24\n",
      "Mumbai              22\n",
      "Name: count, dtype: int64\n",
      "location\n",
      "Waco, Texas                     1\n",
      "todaysbigstock.com              1\n",
      "buenos aires argentina          1\n",
      "everydaynigerian@gmail.com      1\n",
      "Surulere Lagos,Home Of Swagg    1\n",
      "MontrÌ©al, QuÌ©bec              1\n",
      "Montreal                        1\n",
      "ÌÏT: 6.4682,3.18287             1\n",
      "Live4Heed??                     1\n",
      "Lincoln                         1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Locations\n",
    "print(df['location'].value_counts().head(10))\n",
    "print(df['location'].value_counts().tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "id": "6399305c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43 is the percentage of disaster tweets with location\n",
      "0.42 is the percentage of disaster tweets without location\n"
     ]
    }
   ],
   "source": [
    "# Percentage of disaster tweets with and without location\n",
    "df_location = df[df['location'].notna()]\n",
    "df_location_nan = df[df['location'].isna()]\n",
    "print(f\"{round(len(df_location[df_location['target']== 1])/len(df_location),2)} is the percentage of disaster tweets with location\")\n",
    "print(f\"{round(len(df_location_nan[df_location_nan['target']== 1])/len(df_location_nan),2)} is the percentage of disaster tweets without location\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "id": "3ce977b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keyword\n",
      "fatalities     45\n",
      "deluge         42\n",
      "armageddon     42\n",
      "sinking        41\n",
      "damage         41\n",
      "harm           41\n",
      "body%20bags    41\n",
      "outbreak       40\n",
      "evacuate       40\n",
      "fear           40\n",
      "Name: count, dtype: int64\n",
      "keyword\n",
      "volcano                  27\n",
      "battle                   26\n",
      "bush%20fires             25\n",
      "war%20zone               24\n",
      "rescue                   22\n",
      "forest%20fire            19\n",
      "epicentre                12\n",
      "threat                   11\n",
      "inundation               10\n",
      "radiation%20emergency     9\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Keywords\n",
    "print(df['keyword'].value_counts().head(10))\n",
    "print(df['keyword'].value_counts().tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "id": "0c2cdc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4018300248 is the percentage of disaster tweets without % in keyword\n",
      "0.5836909871 is the percentage of disaster tweets with % in keyword\n"
     ]
    }
   ],
   "source": [
    "# Percentage of disaster tweets with and without % in keywords\n",
    "df_keyword_normal = df[~df['keyword'].str.contains('%', regex=False, na=False)]\n",
    "df_keyword_special = df[df['keyword'].str.contains('%', regex=False, na=False)]\n",
    "print(f\"{round(len(df_keyword_normal[df_keyword_normal['target']== 1])/len(df_keyword_normal),10)} is the percentage of disaster tweets without % in keyword\")\n",
    "print(f\"{round(len(df_keyword_special[df_keyword_special['target']== 1])/len(df_keyword_special),10)} is the percentage of disaster tweets with % in keyword\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb38dd6",
   "metadata": {},
   "source": [
    "df[per_in_keyword\"] = 1 if df[~df['keyword'].str.contains('%', regex=False, na=False)] else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cec8bb",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "id": "1c87d66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "contains_percent = df['keyword'].str.contains('%', regex=False, na=False)\n",
    "\n",
    "# Convert boolean True/False to integer 1/0 and assign to new column\n",
    "df['per_in_keyword'] = contains_percent.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2487f76d",
   "metadata": {},
   "source": [
    "## NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b5b0eb",
   "metadata": {},
   "source": [
    "### Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "id": "ec270f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].str.lower()\n",
    "df['text'] = df['text'].str.replace(r'@\\w+', '', regex=True)        # Remove @mentions\n",
    "df['text'] = df['text'].str.replace(r'#\\w+', '', regex=True)        # Remove hashtags\n",
    "df['text'] = df['text'].str.replace(r'http\\S+', '', regex=True)     # Remove URLs\n",
    "df['text'] = df['text'].str.replace(r'[^a-z\\s]', '', regex=True)    # Remove punctuation/numbers\n",
    "df['text'] = df['text'].str.replace(r'[\\u00A0\\u3000]', ' ', regex=True) # Remove special space types\n",
    "df['text'] = df['text'].str.replace(r'\\s+', ' ', regex=True)        # Normalize all whitespace\n",
    "df['text'] = df['text'].str.strip()                                 # Final trim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "id": "8e318b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Check for leading/trailing spaces in any row\n",
    "print(df['text'].apply(lambda x: x.startswith(' ') or x.endswith(' ')).sum())\n",
    "\n",
    "# Check for multiple spaces inside any row\n",
    "print(df['text'].apply(lambda x: '  ' in x).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dff713",
   "metadata": {},
   "source": [
    "### Tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "id": "cd399ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/suzukikenta/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "df['tokens'] = df['text'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad71061",
   "metadata": {},
   "source": [
    "### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "id": "0a49a8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/suzukikenta/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['tokens'] = df['tokens'].apply(lambda tokens: [w for w in tokens if w not in stop_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3664b510",
   "metadata": {},
   "source": [
    "### Lemitizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "id": "e8a8744b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/suzukikenta/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/suzukikenta/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_tokens(tokens):\n",
    "    return [lemmatizer.lemmatize(token, pos='v') for token in tokens]\n",
    "df['tokens'] = df['tokens'].apply(lemmatize_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb54a51",
   "metadata": {},
   "source": [
    "# Train_Test_Split,Modeling and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "id": "bd7253dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.79 is a f1 score of the base model\n"
     ]
    }
   ],
   "source": [
    "# Turn list of tokens into text\n",
    "df['clean_text'] = df['tokens'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "X = df[['clean_text','per_in_keyword']]\n",
    "y = df['target']  # or your label column\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#TF-IDF\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train['clean_text'])\n",
    "X_test_tfidf = vectorizer.transform(X_test['clean_text'])\n",
    "\n",
    "model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "print(f\"{round(f1,2)} is a f1 score of the base model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "id": "9c24152b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - F1 Score: 0.7913\n",
      "Logistic Regression\n",
      "Naive Bayes - F1 Score: 0.7988\n",
      "Naive Bayes\n",
      "Random Forest - F1 Score: 0.7763\n",
      "Random Forest\n",
      "SVM - F1 Score: 0.7677\n",
      "SVM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/suzukikenta/.pyenv/versions/3.12.9/envs/lewagon/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [07:12:21] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost - F1 Score: 0.7687\n",
      "XGBoost\n",
      "Best model: Naive Bayes with F1 score: 0.7988\n"
     ]
    }
   ],
   "source": [
    "# Try different models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Naive Bayes\": MultinomialNB(),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"SVM\": LinearSVC(),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "}\n",
    "\n",
    "my_dict = defaultdict(list)\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "    y_pred = model.predict(X_test_tfidf)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    print(f\"{name} - F1 Score: {f1:.4f}\")\n",
    "    print(str(name))\n",
    "    my_dict[str(name)].append(f1)\n",
    "\n",
    "# Convert defaultdict to regular dict (optional)\n",
    "results = dict(my_dict)\n",
    "\n",
    "# Find the model with the highest F1 score\n",
    "best_model = None\n",
    "best_f1 = -1\n",
    "\n",
    "for model_name, f1_list in results.items():\n",
    "    # Assuming each list has one or more scores, take max\n",
    "    max_f1 = max(f1_list)\n",
    "    if max_f1 > best_f1:\n",
    "        best_f1 = max_f1\n",
    "        best_model = model_name\n",
    "\n",
    "print(f\"Best model: {best_model} with F1 score: {best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad6ed8d",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "id": "e19dce8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Base Naive Bayes F1 Score: 0.7988\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n",
      "GridSearchCV Best Params: {'alpha': 1.0}\n",
      "GridSearch Naive Bayes F1 Score on Test: 0.7988\n"
     ]
    }
   ],
   "source": [
    "# 1. Train base Naive Bayes model\n",
    "base_nb = MultinomialNB()\n",
    "base_nb.fit(X_train_tfidf, y_train)\n",
    "y_pred_base = base_nb.predict(X_test_tfidf)\n",
    "base_f1 = f1_score(y_test, y_pred_base, average='weighted')\n",
    "print(f\"📦 Base Naive Bayes F1 Score: {base_f1:.4f}\")\n",
    "\n",
    "# 2. GridSearchCV with expanded alpha range\n",
    "param_grid = {\n",
    "    'alpha': [0.01, 0.05, 0.1, 0.5, 1.0, 2.0, 5.0]\n",
    "}\n",
    "grid = GridSearchCV(\n",
    "    MultinomialNB(),\n",
    "    param_grid=param_grid,\n",
    "    scoring='f1_weighted',\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "grid.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# 3. Best model evaluation on test set\n",
    "best_nb = grid.best_estimator_\n",
    "y_pred_grid = best_nb.predict(X_test_tfidf)\n",
    "grid_f1 = f1_score(y_test, y_pred_grid, average='weighted')\n",
    "print(f\"GridSearchCV Best Params: {grid.best_params_}\")\n",
    "print(f\"GridSearch Naive Bayes F1 Score on Test: {grid_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e969f2d",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "id": "aba98e9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>10861</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>10865</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Storm in RI worse than last hurricane. My city...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>10868</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Green Line derailment in Chicago http://t.co/U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>10874</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MEG issues Hazardous Weather Outlook (HWO) htt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>10875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#CityofCalgary has activated its Municipal Eme...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         0     NaN      NaN   \n",
       "1         2     NaN      NaN   \n",
       "2         3     NaN      NaN   \n",
       "3         9     NaN      NaN   \n",
       "4        11     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "3258  10861     NaN      NaN   \n",
       "3259  10865     NaN      NaN   \n",
       "3260  10868     NaN      NaN   \n",
       "3261  10874     NaN      NaN   \n",
       "3262  10875     NaN      NaN   \n",
       "\n",
       "                                                   text  \n",
       "0                    Just happened a terrible car crash  \n",
       "1     Heard about #earthquake is different cities, s...  \n",
       "2     there is a forest fire at spot pond, geese are...  \n",
       "3              Apocalypse lighting. #Spokane #wildfires  \n",
       "4         Typhoon Soudelor kills 28 in China and Taiwan  \n",
       "...                                                 ...  \n",
       "3258  EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...  \n",
       "3259  Storm in RI worse than last hurricane. My city...  \n",
       "3260  Green Line derailment in Chicago http://t.co/U...  \n",
       "3261  MEG issues Hazardous Weather Outlook (HWO) htt...  \n",
       "3262  #CityofCalgary has activated its Municipal Eme...  \n",
       "\n",
       "[3263 rows x 4 columns]"
      ]
     },
     "execution_count": 620,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('./twitter/test.csv')\n",
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65db8ea",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "id": "c9686fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/suzukikenta/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/suzukikenta/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/suzukikenta/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/suzukikenta/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "contains_percent = df_test['keyword'].str.contains('%', regex=False, na=False)\n",
    "\n",
    "# Convert boolean True/False to integer 1/0 and assign to new column\n",
    "df_test['per_in_keyword'] = contains_percent.astype(int)\n",
    "\n",
    "df_test['text'] = df_test['text'].str.lower()\n",
    "df_test['text'] = df_test['text'].str.replace(r'@\\w+', '', regex=True)        # Remove @mentions\n",
    "df_test['text'] = df_test['text'].str.replace(r'#\\w+', '', regex=True)        # Remove hashtags\n",
    "df_test['text'] = df_test['text'].str.replace(r'http\\S+', '', regex=True)     # Remove URLs\n",
    "df_test['text'] = df_test['text'].str.replace(r'[^a-z\\s]', '', regex=True)    # Remove punctuation/numbers\n",
    "df_test['text'] = df_test['text'].str.replace(r'[\\u00A0\\u3000]', ' ', regex=True) # Remove special space types\n",
    "df_test['text'] = df_test['text'].str.replace(r'\\s+', ' ', regex=True)        # Normalize all whitespace\n",
    "df_test['text'] = df_test['text'].str.strip()                                 # Final trim\n",
    "\n",
    "# Optional checks (can remove later)\n",
    "print(df_test['text'].apply(lambda x: x.startswith(' ') or x.endswith(' ')).sum())\n",
    "print(df_test['text'].apply(lambda x: '  ' in x).sum())\n",
    "\n",
    "# Tokenization\n",
    "nltk.download('punkt')\n",
    "df_test['tokens'] = df_test['text'].apply(word_tokenize)\n",
    "\n",
    "# Stopword removal\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df_test['tokens'] = df_test['tokens'].apply(lambda tokens: [w for w in tokens if w not in stop_words])\n",
    "\n",
    "# Lemmatization\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_tokens(tokens):\n",
    "    return [lemmatizer.lemmatize(token, pos='v') for token in tokens]\n",
    "df_test['tokens'] = df_test['tokens'].apply(lemmatize_tokens)\n",
    "\n",
    "# Join back into a cleaned string\n",
    "df_test['clean_text'] = df_test['tokens'].apply(lambda x: ' '.join(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "id": "036e5b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Predictions saved to 'submission.csv'\n"
     ]
    }
   ],
   "source": [
    "# Create a submission file\n",
    "final_model = models[best_model]\n",
    "\n",
    "X_final_test = vectorizer.transform(df_test['clean_text'])\n",
    "y_test_pred = final_model.predict(X_final_test)\n",
    "submission = df_test[['id']].copy()\n",
    "submission['target'] = y_test_pred\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"✅ Predictions saved to 'submission.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lewagon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
